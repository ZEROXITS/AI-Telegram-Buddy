[Bot]
token = 

[Ai]
prompt = chat transcript between a human and a friendly bot named Buddy.

# CASE SENSITIVE. Please refer to the end of this document for a list of supported models.
model = Vicuna
model_type = llama
model_size = 7B

# Are you using an uncensored version of the model? yes/no
uncensored = yes

# Instruction set for the bot. If you don't have an avx2 capable CPU, try to switch to "avx".
# If you're using an M1 Mac, use "basic" instead.
# If you're on any other ARM processor, you need to manually compile libctransformers.so and specify the path
instruction_set = avx2

[Chat]

# Delay, in seconds, on how many times the bot will gradually edit the message.
# Warning: anything lower than 3 seconds will eventually trigger the flood control system of telegram APIs.
# Use a lower value only if you can achieve a very fast inference.
edit_delay = 4

# Number of parallel chats that can be processed simultaneously
max_parallel_chats = 3



# LIST OF AVAILABLE MODELS (Remember to download your desired model(s) from model-downloader.py)

# model = Vicuna
# model_type = llama

# model = MPT
# model_type = mpt

# model = GPT4All
# model_type = llama

# model = dolly-v2
# model_type = dolly-v2

# model = LLaMa
# model_type = llama